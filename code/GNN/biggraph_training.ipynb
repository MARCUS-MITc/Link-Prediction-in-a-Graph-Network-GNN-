{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found some files that indicate that the input data has already been shuffled and split, not doing it again.\n",
      "These files are: data/statml/train.txt and data/statml/test.txt\n",
      "Using the 1 relation types given in the config\n",
      "Searching for the entities in the edge files...\n",
      "Entity type user_id:\n",
      "- Found 4867134 entities\n",
      "- Removing the ones with fewer than 1 occurrences...\n",
      "- Left with 4867134 entities\n",
      "- Shuffling them...\n",
      "Preparing counts and dictionaries for entities and relation types:\n",
      "- Writing count of entity type user_id and partition 0\n",
      "Preparing edge path data/train_partitioned, out of the edges found in data/statml/train.txt\n",
      "- Edges will be partitioned in 1 x 1 buckets.\n",
      "- Processed 100000 edges so far...\n",
      "- Processed 200000 edges so far...\n",
      "- Processed 300000 edges so far...\n",
      "- Processed 400000 edges so far...\n",
      "- Processed 500000 edges so far...\n",
      "- Processed 600000 edges so far...\n",
      "- Processed 700000 edges so far...\n",
      "- Processed 800000 edges so far...\n",
      "- Processed 900000 edges so far...\n",
      "- Processed 1000000 edges so far...\n",
      "- Processed 1100000 edges so far...\n",
      "- Processed 1200000 edges so far...\n",
      "- Processed 1300000 edges so far...\n",
      "- Processed 1400000 edges so far...\n",
      "- Processed 1500000 edges so far...\n",
      "- Processed 1600000 edges so far...\n",
      "- Processed 1700000 edges so far...\n",
      "- Processed 1800000 edges so far...\n",
      "- Processed 1900000 edges so far...\n",
      "- Processed 2000000 edges so far...\n",
      "- Processed 2100000 edges so far...\n",
      "- Processed 2200000 edges so far...\n",
      "- Processed 2300000 edges so far...\n",
      "- Processed 2400000 edges so far...\n",
      "- Processed 2500000 edges so far...\n",
      "- Processed 2600000 edges so far...\n",
      "- Processed 2700000 edges so far...\n",
      "- Processed 2800000 edges so far...\n",
      "- Processed 2900000 edges so far...\n",
      "- Processed 3000000 edges so far...\n",
      "- Processed 3100000 edges so far...\n",
      "- Processed 3200000 edges so far...\n",
      "- Processed 3300000 edges so far...\n",
      "- Processed 3400000 edges so far...\n",
      "- Processed 3500000 edges so far...\n",
      "- Processed 3600000 edges so far...\n",
      "- Processed 3700000 edges so far...\n",
      "- Processed 3800000 edges so far...\n",
      "- Processed 3900000 edges so far...\n",
      "- Processed 4000000 edges so far...\n",
      "- Processed 4100000 edges so far...\n",
      "- Processed 4200000 edges so far...\n",
      "- Processed 4300000 edges so far...\n",
      "- Processed 4400000 edges so far...\n",
      "- Processed 4500000 edges so far...\n",
      "- Processed 4600000 edges so far...\n",
      "- Processed 4700000 edges so far...\n",
      "- Processed 4800000 edges so far...\n",
      "- Processed 4900000 edges so far...\n",
      "- Processed 5000000 edges so far...\n",
      "- Processed 5100000 edges so far...\n",
      "- Processed 5200000 edges so far...\n",
      "- Processed 5300000 edges so far...\n",
      "- Processed 5400000 edges so far...\n",
      "- Processed 5500000 edges so far...\n",
      "- Processed 5600000 edges so far...\n",
      "- Processed 5700000 edges so far...\n",
      "- Processed 5800000 edges so far...\n",
      "- Processed 5900000 edges so far...\n",
      "- Processed 6000000 edges so far...\n",
      "- Processed 6100000 edges so far...\n",
      "- Processed 6200000 edges so far...\n",
      "- Processed 6300000 edges so far...\n",
      "- Processed 6400000 edges so far...\n",
      "- Processed 6500000 edges so far...\n",
      "- Processed 6600000 edges so far...\n",
      "- Processed 6700000 edges so far...\n",
      "- Processed 6800000 edges so far...\n",
      "- Processed 6900000 edges so far...\n",
      "- Processed 7000000 edges so far...\n",
      "- Processed 7100000 edges so far...\n",
      "- Processed 7200000 edges so far...\n",
      "- Processed 7300000 edges so far...\n",
      "- Processed 7400000 edges so far...\n",
      "- Processed 7500000 edges so far...\n",
      "- Processed 7600000 edges so far...\n",
      "- Processed 7700000 edges so far...\n",
      "- Processed 7800000 edges so far...\n",
      "- Processed 7900000 edges so far...\n",
      "- Processed 8000000 edges so far...\n",
      "- Processed 8100000 edges so far...\n",
      "- Processed 8200000 edges so far...\n",
      "- Processed 8300000 edges so far...\n",
      "- Processed 8400000 edges so far...\n",
      "- Processed 8500000 edges so far...\n",
      "- Processed 8600000 edges so far...\n",
      "- Processed 8700000 edges so far...\n",
      "- Processed 8800000 edges so far...\n",
      "- Processed 8900000 edges so far...\n",
      "- Processed 9000000 edges so far...\n",
      "- Processed 9100000 edges so far...\n",
      "- Processed 9200000 edges so far...\n",
      "- Processed 9300000 edges so far...\n",
      "- Processed 9400000 edges so far...\n",
      "- Processed 9500000 edges so far...\n",
      "- Processed 9600000 edges so far...\n",
      "- Processed 9700000 edges so far...\n",
      "- Processed 9800000 edges so far...\n",
      "- Processed 9900000 edges so far...\n",
      "- Processed 10000000 edges so far...\n",
      "- Processed 10100000 edges so far...\n",
      "- Processed 10200000 edges so far...\n",
      "- Processed 10300000 edges so far...\n",
      "- Processed 10400000 edges so far...\n",
      "- Processed 10500000 edges so far...\n",
      "- Processed 10600000 edges so far...\n",
      "- Processed 10700000 edges so far...\n",
      "- Processed 10800000 edges so far...\n",
      "- Processed 10900000 edges so far...\n",
      "- Processed 11000000 edges so far...\n",
      "- Processed 11100000 edges so far...\n",
      "- Processed 11200000 edges so far...\n",
      "- Processed 11300000 edges so far...\n",
      "- Processed 11400000 edges so far...\n",
      "- Processed 11500000 edges so far...\n",
      "- Processed 11600000 edges so far...\n",
      "- Processed 11700000 edges so far...\n",
      "- Processed 11800000 edges so far...\n",
      "- Processed 11900000 edges so far...\n",
      "- Processed 12000000 edges so far...\n",
      "- Processed 12100000 edges so far...\n",
      "- Processed 12200000 edges so far...\n",
      "- Processed 12300000 edges so far...\n",
      "- Processed 12400000 edges so far...\n",
      "- Processed 12500000 edges so far...\n",
      "- Processed 12600000 edges so far...\n",
      "- Processed 12700000 edges so far...\n",
      "- Processed 12800000 edges so far...\n",
      "- Processed 12900000 edges so far...\n",
      "- Processed 13000000 edges so far...\n",
      "- Processed 13100000 edges so far...\n",
      "- Processed 13200000 edges so far...\n",
      "- Processed 13300000 edges so far...\n",
      "- Processed 13400000 edges so far...\n",
      "- Processed 13500000 edges so far...\n",
      "- Processed 13600000 edges so far...\n",
      "- Processed 13700000 edges so far...\n",
      "- Processed 13800000 edges so far...\n",
      "- Processed 13900000 edges so far...\n",
      "- Processed 14000000 edges so far...\n",
      "- Processed 14100000 edges so far...\n",
      "- Processed 14200000 edges so far...\n",
      "- Processed 14300000 edges so far...\n",
      "- Processed 14400000 edges so far...\n",
      "- Processed 14500000 edges so far...\n",
      "- Processed 14600000 edges so far...\n",
      "- Processed 14700000 edges so far...\n",
      "- Processed 14800000 edges so far...\n",
      "- Processed 14900000 edges so far...\n",
      "- Processed 15000000 edges so far...\n",
      "- Processed 15100000 edges so far...\n",
      "- Processed 15200000 edges so far...\n",
      "- Processed 15300000 edges so far...\n",
      "- Processed 15400000 edges so far...\n",
      "- Processed 15500000 edges so far...\n",
      "- Processed 15600000 edges so far...\n",
      "- Processed 15700000 edges so far...\n",
      "- Processed 15800000 edges so far...\n",
      "- Processed 15900000 edges so far...\n",
      "- Processed 16000000 edges so far...\n",
      "- Processed 16100000 edges so far...\n",
      "- Processed 16200000 edges so far...\n",
      "- Processed 16300000 edges so far...\n",
      "- Processed 16400000 edges so far...\n",
      "- Processed 16500000 edges so far...\n",
      "- Processed 16600000 edges so far...\n",
      "- Processed 16700000 edges so far...\n",
      "- Processed 16800000 edges so far...\n",
      "- Processed 16900000 edges so far...\n",
      "- Processed 17000000 edges so far...\n",
      "- Processed 17100000 edges so far...\n",
      "- Processed 17200000 edges so far...\n",
      "- Processed 17300000 edges so far...\n",
      "- Processed 17400000 edges so far...\n",
      "- Processed 17500000 edges so far...\n",
      "- Processed 17600000 edges so far...\n",
      "- Processed 17700000 edges so far...\n",
      "- Processed 17800000 edges so far...\n",
      "- Processed 17900000 edges so far...\n",
      "- Processed 17959948 edges in total\n",
      "Preparing edge path data/test_partitioned, out of the edges found in data/statml/test.txt\n",
      "- Edges will be partitioned in 1 x 1 buckets.\n",
      "- Processed 100000 edges so far...\n",
      "- Processed 200000 edges so far...\n",
      "- Processed 300000 edges so far...\n",
      "- Processed 400000 edges so far...\n",
      "- Processed 500000 edges so far...\n",
      "- Processed 600000 edges so far...\n",
      "- Processed 700000 edges so far...\n",
      "- Processed 800000 edges so far...\n",
      "- Processed 900000 edges so far...\n",
      "- Processed 1000000 edges so far...\n",
      "- Processed 1100000 edges so far...\n",
      "- Processed 1200000 edges so far...\n",
      "- Processed 1300000 edges so far...\n",
      "- Processed 1400000 edges so far...\n",
      "- Processed 1500000 edges so far...\n",
      "- Processed 1600000 edges so far...\n",
      "- Processed 1700000 edges so far...\n",
      "- Processed 1800000 edges so far...\n",
      "- Processed 1900000 edges so far...\n",
      "- Processed 2000000 edges so far...\n",
      "- Processed 2100000 edges so far...\n",
      "- Processed 2200000 edges so far...\n",
      "- Processed 2300000 edges so far...\n",
      "- Processed 2400000 edges so far...\n",
      "- Processed 2500000 edges so far...\n",
      "- Processed 2600000 edges so far...\n",
      "- Processed 2700000 edges so far...\n",
      "- Processed 2800000 edges so far...\n",
      "- Processed 2900000 edges so far...\n",
      "- Processed 3000000 edges so far...\n",
      "- Processed 3100000 edges so far...\n",
      "- Processed 3200000 edges so far...\n",
      "- Processed 3300000 edges so far...\n",
      "- Processed 3400000 edges so far...\n",
      "- Processed 3500000 edges so far...\n",
      "- Processed 3600000 edges so far...\n",
      "- Processed 3700000 edges so far...\n",
      "- Processed 3800000 edges so far...\n",
      "- Processed 3900000 edges so far...\n",
      "- Processed 4000000 edges so far...\n",
      "- Processed 4100000 edges so far...\n",
      "- Processed 4200000 edges so far...\n",
      "- Processed 4300000 edges so far...\n",
      "- Processed 4400000 edges so far...\n",
      "- Processed 4500000 edges so far...\n",
      "- Processed 4600000 edges so far...\n",
      "- Processed 4700000 edges so far...\n",
      "- Processed 4800000 edges so far...\n",
      "- Processed 4900000 edges so far...\n",
      "- Processed 5000000 edges so far...\n",
      "- Processed 5100000 edges so far...\n",
      "- Processed 5200000 edges so far...\n",
      "- Processed 5300000 edges so far...\n",
      "- Processed 5400000 edges so far...\n",
      "- Processed 5500000 edges so far...\n",
      "- Processed 5600000 edges so far...\n",
      "- Processed 5700000 edges so far...\n",
      "- Processed 5800000 edges so far...\n",
      "- Processed 5900000 edges so far...\n",
      "- Processed 5986650 edges in total\n",
      "2020-09-11 11:17:59,041   [Trainer-0] Loading entity counts...\n",
      "2020-09-11 11:17:59,043   [Trainer-0] Creating workers...\n",
      "2020-09-11 11:17:59,223   [Trainer-0] Initializing global model...\n",
      "2020-09-11 11:18:24,765   [Trainer-0] Starting epoch 1 / 30, edge path 1 / 1, edge chunk 1 / 1\n",
      "2020-09-11 11:18:24,766   [Trainer-0] Edge path: data/train_partitioned\n",
      "2020-09-11 11:18:24,768   [Trainer-0] still in queue: 0\n",
      "2020-09-11 11:18:24,768   [Trainer-0] Swapping partitioned embeddings None ( 0 , 0 )\n",
      "2020-09-11 11:18:24,769   [Trainer-0] ( 0 , 0 ): Loading entities\n",
      "2020-09-11 11:18:53,445   [Trainer-0] ( 0 , 0 ): Stats before training: pos_rank:  1000.03 , mrr:  0.00399191 , r1:  0.000427618 , r10:  0.00484356 , r50:  0.0247106 , auc:  0.501203 , count:  897997\n",
      "2020-09-11 11:25:51,593   [Trainer-0] ( 0 , 0 ): bucket 1 / 1 : Processed 17061951 edges in 418.14 s ( 0.041 M/sec ); io: 28.68 s ( 15.03 MB/sec )\n",
      "2020-09-11 11:25:51,594   [Trainer-0] ( 0 , 0 ): loss:  10.8638 , violators_lhs:  23.4203 , violators_rhs:  23.7172 , count:  17061951\n",
      "2020-09-11 11:26:10,727   [Trainer-0] ( 0 , 0 ): Stats after training: pos_rank:  362.612 , mrr:  0.0440328 , r1:  0.0153068 , r10:  0.0893583 , r50:  0.256337 , auc:  0.822012 , count:  897997\n",
      "2020-09-11 11:26:10,728   [Trainer-0] Swapping partitioned embeddings ( 0 , 0 ) None\n",
      "2020-09-11 11:26:10,730   [Trainer-0] Writing partitioned embeddings\n",
      "2020-09-11 11:26:10,731   [Trainer-0] Finished epoch 1 / 30, edge path 1 / 1, edge chunk 1 / 1\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the BSD-style license found in the\n",
    "# LICENSE.txt file in the root directory of this source tree.\n",
    "\n",
    "import argparse\n",
    "import random\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "\n",
    "import attr\n",
    "import pkg_resources\n",
    "\n",
    "from torchbiggraph.config import add_to_sys_path, ConfigFileLoader\n",
    "from torchbiggraph.converters.import_from_tsv import convert_input_data\n",
    "from torchbiggraph.converters.utils import download_url, extract_gzip\n",
    "from torchbiggraph.eval import do_eval\n",
    "from torchbiggraph.train import train\n",
    "from torchbiggraph.util import (\n",
    "    set_logging_verbosity,\n",
    "    setup_logging,\n",
    "    SubprocessInitializer,\n",
    ")\n",
    "\n",
    "\n",
    "TRAIN_FILENAME = \"train.txt\"\n",
    "TEST_FILENAME = \"test.txt\"\n",
    "FILENAMES = [\n",
    "    TRAIN_FILENAME,\n",
    "    TEST_FILENAME,\n",
    "]\n",
    "TRAIN_FRACTION = 0.75\n",
    "\n",
    "# Figure out the path where the sample config was installed by the package manager.\n",
    "# This can be overridden with --config.\n",
    "DEFAULT_CONFIG = './config/config_50_epochs_logistic.py'\n",
    "\n",
    "\n",
    "def random_split_file(fpath: Path) -> None:\n",
    "    train_file = fpath.parent / TRAIN_FILENAME\n",
    "    test_file = fpath.parent / TEST_FILENAME\n",
    "\n",
    "    if train_file.exists() and test_file.exists():\n",
    "        print(\"Found some files that indicate that the input data \"\n",
    "              \"has already been shuffled and split, not doing it again.\")\n",
    "        print(f\"These files are: {train_file} and {test_file}\")\n",
    "        return\n",
    "\n",
    "    print('Shuffling and splitting train/test file. This may take a while.')\n",
    "\n",
    "    print(f\"Reading data from file: {fpath}\")\n",
    "    with fpath.open(\"rt\") as in_tf:\n",
    "        lines = in_tf.readlines()\n",
    "\n",
    "    # The first few lines are comments\n",
    "    lines = lines[4:]\n",
    "    print('Shuffling data')\n",
    "    random.shuffle(lines)\n",
    "    split_len = int(len(lines) * TRAIN_FRACTION)\n",
    "\n",
    "    print('Splitting to train and test files')\n",
    "    with train_file.open(\"wt\") as out_tf_train:\n",
    "        for line in lines[:split_len]:\n",
    "            out_tf_train.write(line)\n",
    "\n",
    "    with test_file.open(\"wt\") as out_tf_test:\n",
    "        for line in lines[split_len:]:\n",
    "            out_tf_test.write(line)\n",
    "\n",
    "\n",
    "def main():\n",
    "    setup_logging()\n",
    "\n",
    "\n",
    "\n",
    "    # download data\n",
    "    path = './data/statml/'\n",
    "    data_dir = Path(path)\n",
    "    \n",
    "    training_data = './data/statml/individual_links.txt'\n",
    "    fpath = Path(training_data)\n",
    "\n",
    "    # random split file for train and test\n",
    "    random_split_file(fpath)\n",
    "\n",
    "    loader = ConfigFileLoader()\n",
    "    config = loader.load_config(DEFAULT_CONFIG, overrides=None)\n",
    "    set_logging_verbosity(config.verbose)\n",
    "    subprocess_init = SubprocessInitializer()\n",
    "    subprocess_init.register(setup_logging, config.verbose)\n",
    "    subprocess_init.register(add_to_sys_path, loader.config_dir.name)\n",
    "    input_edge_paths = [data_dir / name for name in FILENAMES]\n",
    "    output_train_path, output_test_path = config.edge_paths\n",
    "\n",
    "    convert_input_data(\n",
    "        config.entities,\n",
    "        config.relations,\n",
    "        config.entity_path,\n",
    "        config.edge_paths,\n",
    "        input_edge_paths,\n",
    "        lhs_col=0,\n",
    "        rhs_col=1,\n",
    "        rel_col=None,\n",
    "        dynamic_relations=config.dynamic_relations,\n",
    "    )\n",
    "\n",
    "    train_config = attr.evolve(config, edge_paths=[output_train_path])\n",
    "    train(train_config, subprocess_init=subprocess_init)\n",
    "\n",
    "    eval_config = attr.evolve(config, edge_paths=[output_test_path])\n",
    "    do_eval(eval_config, subprocess_init=subprocess_init)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf-gpu.1-15.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf-gpu.1-15:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
